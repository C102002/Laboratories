{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nseFeD-DSdv"
      },
      "source": [
        "# Laboratorio 3 Evaluado\n",
        "## Integrantes\n",
        "* Daniel Bortot\n",
        "* Hualong Chiang\n",
        "* Alfredo Fung\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KEbRsYZFny"
      },
      "source": [
        "# Situación:\n",
        "\n",
        "Gustavo no quiere seguir trabajando por una miseria de sueldo y ha decidido embarcarse en una iniciativa relacionada a finanzas. Este negocio consiste en ofrecer prestamos para personas que quieren comenzar negocios profesionales. Solo hay un problema... Gustavo no sabe nada del tema.\n",
        "\n",
        "Pero no pasa nada, en un mundo post IA puede dejarle el trabajo a un modelo de inteligencia artificial, entonces los ha contratado para que diseñen un modelo que sea capaz de predecir si es seguro prestarle dinero a una persona o no.\n",
        "\n",
        "Para echarles un cable, Gustavo consiguio una base de datos sobre [prestamos](https://drive.google.com/file/d/1toUeiwOyEdmFgrtz84DxgY6y1i9-XHny/view?usp=sharing) a ciudadanos americanos para que lo ayuden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEEhzT8ycJT6"
      },
      "source": [
        "Objetivo del laboratorio:\n",
        "\n",
        "Diseñar un modelo MLP (MultiLayer Perceptron) que sea capaz de determinar si se debe aprobar o no un prestamo a una persona. Adicionalmente, la entrega final debe incluir un programa que el usuario pueda utilizar para ingresar los datos claves y en función a ese input devolver una recomendación.\n",
        "\n",
        "Por ejemplo algo así:\n",
        "\n",
        "\"Por favor escriba (en numeros) x variable:\"\n",
        "\n",
        "Usuario escribe Y.\n",
        "\n",
        "\"En base a la información ustedes deberia... \"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43oSSaJfJyMy"
      },
      "source": [
        "## 1. Instalamos Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EbSQYwNPJvTq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from scikit-learn) (2.3.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from pandas) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (4.14.0)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached grpcio-1.72.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alfni\\onedrive\\desktop\\ia\\laboratories\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
            "Using cached grpcio-1.72.1-cp311-cp311-win_amd64.whl (4.2 MB)\n",
            "Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
            "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
            "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
            "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, wheel, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, numpy, MarkupSafe, markdown, grpcio, google-pasta, gast, werkzeug, astunparse, tensorboard, tensorflow\n",
            "\n",
            "   ----------------------------------------  0/19 [libclang]\n",
            "   ----------------------------------------  0/19 [libclang]\n",
            "   ----------------------------------------  0/19 [libclang]\n",
            "   ----------------------------------------  0/19 [libclang]\n",
            "   ----------------------------------------  0/19 [libclang]\n",
            "   -- -------------------------------------  1/19 [flatbuffers]\n",
            "   ------ ---------------------------------  3/19 [wheel]\n",
            "   ------------ ---------------------------  6/19 [tensorboard-data-server]\n",
            "   -------------- -------------------------  7/19 [protobuf]\n",
            "   -------------- -------------------------  7/19 [protobuf]\n",
            "   ---------------- -----------------------  8/19 [opt-einsum]\n",
            "   ---------------- -----------------------  8/19 [opt-einsum]\n",
            "  Attempting uninstall: numpy\n",
            "   ---------------- -----------------------  8/19 [opt-einsum]\n",
            "    Found existing installation: numpy 2.3.0\n",
            "   ---------------- -----------------------  8/19 [opt-einsum]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "    Uninstalling numpy-2.3.0:\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "      Successfully uninstalled numpy-2.3.0\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   ------------------ ---------------------  9/19 [numpy]\n",
            "   --------------------- ------------------ 10/19 [MarkupSafe]\n",
            "   ----------------------- ---------------- 11/19 [markdown]\n",
            "   ----------------------- ---------------- 11/19 [markdown]\n",
            "   ------------------------- -------------- 12/19 [grpcio]\n",
            "   ------------------------- -------------- 12/19 [grpcio]\n",
            "   ------------------------- -------------- 12/19 [grpcio]\n",
            "   ------------------------- -------------- 12/19 [grpcio]\n",
            "   --------------------------- ------------ 13/19 [google-pasta]\n",
            "   ------------------------------- -------- 15/19 [werkzeug]\n",
            "   ------------------------------- -------- 15/19 [werkzeug]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ----------------------------------- ---- 17/19 [tensorboard]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ------------------------------------- -- 18/19 [tensorflow]\n",
            "   ---------------------------------------- 19/19 [tensorflow]\n",
            "\n",
            "Successfully installed MarkupSafe-3.0.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.72.1 libclang-18.1.1 markdown-3.8 numpy-2.1.3 opt-einsum-3.4.0 protobuf-5.29.5 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\alfni\\OneDrive\\Desktop\\IA\\Laboratories\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\alfni\\OneDrive\\Desktop\\IA\\Laboratories\\.venv\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U scikit-learn\n",
        "%pip install -U pandas\n",
        "%pip install -q -U keras-tuner\n",
        "%pip install -U tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RILC8GqfJ6ZZ"
      },
      "source": [
        "## 2. Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "IgKfjf8tFUz8",
        "outputId": "ad90f59f-7b93-4824-85ac-3efc41119bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Columnas del archivo propuesto:\n",
            "Text, Income, Credit_Score, Loan_Amount, DTI_Ratio, Employment_Status, Approval\n",
            "\n",
            "🔎 Palabras clave más frecuentes: ['start', 'invest']\n",
            "\n",
            "✅ DataFrame actualizado con Keyword_Flag:\n",
            "\n",
            "🚀 Mapeo de valores de Employment_Status: Index(['Text', 'Income', 'Credit_Score', 'Loan_Amount', 'DTI_Ratio',\n",
            "       'Employment_Status'],\n",
            "      dtype='object')\n",
            "🚀 Mapeo de valores de Approval: {'Approved': np.int64(0), 'Rejected': np.int64(1)}\n",
            "\n",
            "🔍 Valores de Y codificados: [1 1 1 ... 1 1 0]\n",
            "✅ Cantidad de elementos en Y: 24000\n",
            "🚀 Mapeo de valores de Approval: {'Approved': np.int64(0), 'Rejected': np.int64(1)}\n",
            "\n",
            "📊 Dataframe creado:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Income</th>\n",
              "      <th>Credit_Score</th>\n",
              "      <th>Loan_Amount</th>\n",
              "      <th>DTI_Ratio</th>\n",
              "      <th>Employment_Status</th>\n",
              "      <th>Approval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I need a loan to pay for an international vaca...</td>\n",
              "      <td>26556</td>\n",
              "      <td>581</td>\n",
              "      <td>8314</td>\n",
              "      <td>79.26</td>\n",
              "      <td>employed</td>\n",
              "      <td>Rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I want to make home improvements like installi...</td>\n",
              "      <td>197392</td>\n",
              "      <td>389</td>\n",
              "      <td>111604</td>\n",
              "      <td>22.14</td>\n",
              "      <td>employed</td>\n",
              "      <td>Rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I need a loan for home renovation, including a...</td>\n",
              "      <td>44561</td>\n",
              "      <td>523</td>\n",
              "      <td>34118</td>\n",
              "      <td>45.44</td>\n",
              "      <td>employed</td>\n",
              "      <td>Rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I need funds to buy new furniture and applianc...</td>\n",
              "      <td>190363</td>\n",
              "      <td>729</td>\n",
              "      <td>118757</td>\n",
              "      <td>10.22</td>\n",
              "      <td>unemployed</td>\n",
              "      <td>Rejected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I need a loan to start a small business.</td>\n",
              "      <td>61853</td>\n",
              "      <td>732</td>\n",
              "      <td>19210</td>\n",
              "      <td>44.13</td>\n",
              "      <td>employed</td>\n",
              "      <td>Approved</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Income  Credit_Score  \\\n",
              "0  I need a loan to pay for an international vaca...   26556           581   \n",
              "1  I want to make home improvements like installi...  197392           389   \n",
              "2  I need a loan for home renovation, including a...   44561           523   \n",
              "3  I need funds to buy new furniture and applianc...  190363           729   \n",
              "4           I need a loan to start a small business.   61853           732   \n",
              "\n",
              "   Loan_Amount  DTI_Ratio Employment_Status  Approval  \n",
              "0         8314      79.26          employed  Rejected  \n",
              "1       111604      22.14          employed  Rejected  \n",
              "2        34118      45.44          employed  Rejected  \n",
              "3       118757      10.22        unemployed  Rejected  \n",
              "4        19210      44.13          employed  Approved  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "dataframe = pd.read_csv(\"./data/loan_data.csv\")\n",
        "print(f\"\\n📌 Columnas del archivo propuesto:\\n{', '.join(dataframe.columns)}\\n\")\n",
        "\n",
        "# Extraemos los datos de X y de Y\n",
        "X = dataframe[['Text', 'Income', 'Credit_Score',\"Loan_Amount\",\"DTI_Ratio\",\"Employment_Status\"]]\n",
        "Y = dataframe['Approval']\n",
        "\n",
        "# Filtrar solo los textos aprobados\n",
        "approved_texts = \" \".join(dataframe[dataframe[\"Approval\"] == \"Approved\"][\"Text\"]).lower()\n",
        "\n",
        "# Limpiar texto (eliminar signos de puntuación)\n",
        "approved_texts = re.sub(r'[^\\w\\s]', '', approved_texts)\n",
        "\n",
        "# Separar palabras y contar frecuencia\n",
        "word_counts = Counter(approved_texts.split())\n",
        "\n",
        "# Otro criterio\n",
        "# Obtener las 5 palabras más comunes de los textos aprobados En caso que se quiera hacer con ese criterio\n",
        "# Si se quiere hacer para los mas comunes\n",
        "\n",
        "# keywords = [word for word, count in word_counts.most_common(5)]\n",
        "\n",
        "keywords=[\"start\", \"invest\"]\n",
        "\n",
        "print(\"🔎 Palabras clave más frecuentes:\", keywords)\n",
        "\n",
        "# Función para marcar si un texto contiene palabras clave\n",
        "def contains_keywords(text, keywords):\n",
        "    text = text.lower()\n",
        "    return 1 if any(word in text for word in keywords) else 0\n",
        "\n",
        "# Funcion para clasificar a las personas de empleados y no empleados\n",
        "def is_employed(person):\n",
        "  if(person==\"employed\"):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# Aplicar la función al DataFrame\n",
        "X = X.copy() # Se coloca la copia para evitar que se lance el error al filtrar los datos\n",
        "X[\"Text\"] = dataframe[\"Text\"].apply(lambda x: contains_keywords(x, keywords))\n",
        "\n",
        "X[\"Employment_Status\"] = dataframe[\"Employment_Status\"].apply(lambda x: is_employed(x))\n",
        "\n",
        "print(\"\\n✅ DataFrame actualizado con Keyword_Flag:\\n\")\n",
        "\n",
        "# Ver los valores asignados a cada estatus de empleo\n",
        "print(\"🚀 Mapeo de valores de Employment_Status:\",X.columns )\n",
        "\n",
        "\n",
        "# Aplicar LabelEncoder\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(Y)\n",
        "\n",
        "# Mostrar mapeo de valores\n",
        "mapping_approval = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"🚀 Mapeo de valores de Approval:\", mapping_approval)\n",
        "\n",
        "# Ver los valores transformados\n",
        "print(\"\\n🔍 Valores de Y codificados:\", Y)\n",
        "print(\"✅ Cantidad de elementos en Y:\", len(Y))\n",
        "\n",
        "# Ver los valores asignados a cada estado de aprobación\n",
        "mapping_approval = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"🚀 Mapeo de valores de Approval:\", mapping_approval)\n",
        "\n",
        "print(\"\\n📊 Dataframe creado:\\n\")\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "ZCeN-H-VL9d3",
        "outputId": "fa71ad2e-636a-4f17-8948-f904e151b93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Dataframe creado (X):\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Income</th>\n",
              "      <th>Credit_Score</th>\n",
              "      <th>Loan_Amount</th>\n",
              "      <th>DTI_Ratio</th>\n",
              "      <th>Employment_Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>26556</td>\n",
              "      <td>581</td>\n",
              "      <td>8314</td>\n",
              "      <td>79.26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>197392</td>\n",
              "      <td>389</td>\n",
              "      <td>111604</td>\n",
              "      <td>22.14</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>44561</td>\n",
              "      <td>523</td>\n",
              "      <td>34118</td>\n",
              "      <td>45.44</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>190363</td>\n",
              "      <td>729</td>\n",
              "      <td>118757</td>\n",
              "      <td>10.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>61853</td>\n",
              "      <td>732</td>\n",
              "      <td>19210</td>\n",
              "      <td>44.13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Text  Income  Credit_Score  Loan_Amount  DTI_Ratio  Employment_Status\n",
              "0     0   26556           581         8314      79.26                  1\n",
              "1     0  197392           389       111604      22.14                  1\n",
              "2     0   44561           523        34118      45.44                  1\n",
              "3     0  190363           729       118757      10.22                  0\n",
              "4     1   61853           732        19210      44.13                  1"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Impresion del dataframe de los valores de X\n",
        "print(\"\\n📊 Dataframe creado (X):\\n\")\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgciO7RwOY-Z",
        "outputId": "f87f3c74-4b36-464c-cebc-b25ce17db5f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Dataframe creado (Y):\n",
            "\n",
            "[1 1 1 ... 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Impresion del dataframe de los valores de Y\n",
        "\n",
        "print(\"\\n📊 Dataframe creado (Y):\\n\")\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0D0BD1JLFp"
      },
      "source": [
        "## 3. Despues de haber cargado los datos se procede a normalizar los datos que hay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "U6-qbbVaJIJ7",
        "outputId": "5af18f09-9ecb-47cb-a054-ce614eef466a"
      },
      "outputs": [],
      "source": [
        "import numpy as np #numpy\n",
        "from sklearn.model_selection import train_test_split #separador de casos para entrenamiento y prueba\n",
        "from tensorflow import keras #Wrapper de tensorflow\n",
        "from tensorflow.keras.models import Sequential #Para hacer modelos secuenciales\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras_tuner.tuners import RandomSearch #Usando el metodo de tuning RandomSearch\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Se normalizan los datos a entrenar\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Converte y to categorical (one-hot encoding)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_dZWCycN1M"
      },
      "source": [
        "## 4. Se procede a entrenar el modelo separando los datos de entrada y salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrsoI52FRNH3",
        "outputId": "a0bb65b0-0cd2-440d-e433-37318d632b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 00m 38s]\n",
            "val_accuracy: 0.9947916865348816\n",
            "\n",
            "Best val_accuracy So Far: 0.9961805542310079\n",
            "Total elapsed time: 00h 02m 43s\n",
            "Best hyperparameters: {'units': 256, 'learning_rate': 0.0044558929106897766}\n",
            "Epoch 1/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9425 - loss: 0.1290 - val_accuracy: 0.9839 - val_loss: 0.0386\n",
            "Epoch 2/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9848 - loss: 0.0364 - val_accuracy: 0.9872 - val_loss: 0.0291\n",
            "Epoch 3/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0268 - val_accuracy: 0.9911 - val_loss: 0.0222\n",
            "Epoch 4/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9928 - loss: 0.0205 - val_accuracy: 0.9940 - val_loss: 0.0166\n",
            "Epoch 5/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9925 - loss: 0.0182 - val_accuracy: 0.9927 - val_loss: 0.0167\n",
            "Epoch 6/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9897 - loss: 0.0227 - val_accuracy: 0.9945 - val_loss: 0.0152\n",
            "Epoch 7/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0170 - val_accuracy: 0.9948 - val_loss: 0.0126\n",
            "Epoch 8/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0160 - val_accuracy: 0.9924 - val_loss: 0.0164\n",
            "Epoch 9/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9925 - loss: 0.0166 - val_accuracy: 0.9966 - val_loss: 0.0104\n",
            "Epoch 10/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0149 - val_accuracy: 0.9943 - val_loss: 0.0169\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9949 - loss: 0.0229\n",
            "Accuracy on test set: 0.9947916865348816\n"
          ]
        }
      ],
      "source": [
        "import numpy as np #numpy\n",
        "from sklearn.model_selection import train_test_split #separador de casos para entrenamiento y prueba\n",
        "from tensorflow import keras #Wrapper de tensorflow\n",
        "from tensorflow.keras.models import Sequential #Para hacer modelos secuenciales\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras_tuner.tuners import RandomSearch #Usando el metodo de tuning RandomSearch\n",
        "\n",
        "def build_model(hp):\n",
        "    # Se define un modelo Secuencial\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "                    activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(2, activation='softmax')) # Output layer with softmax for multi-class\n",
        "    model.compile(optimizer=keras.optimizers.Adam(\n",
        "        hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='my_dir',\n",
        "    project_name='financing_classification')\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters: {best_hps.values}\")\n",
        "\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "best_model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy on test set: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxKK-s7V8eNw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRw6fPp-S8Oi"
      },
      "source": [
        "## 5. Validamos los valores generedados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsPAyuosTDlq",
        "outputId": "874abcf4-2e02-430e-d5b2-6ed8f65e451d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 🔮 **Análisis del Modelo de Predicción de Préstamos** 🔮 📊\n",
            "--------------------------------------------------\n",
            "🔍 Exploración de los datos:\n",
            "✅ Valores únicos en el conjunto de entrenamiento (Y): [0. 1.]\n",
            "\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step\n",
            "📈 Comparación entre Predicción y Realidad del modelo de Aprobacion de prestamos:\n",
            "--------------------------------------------------\n",
            "🔹 Sample 01 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 02 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 03 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 04 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 05 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 06 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 07 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 08 : 🏦 Predicted=0 | Real=0\n",
            "🔹 Sample 09 : 🏦 Predicted=1 | Real=1\n",
            "🔹 Sample 10 : 🏦 Predicted=1 | Real=1\n",
            "\n",
            "--------------------------------------------------\n",
            "📊  Resumen del Rendimiento del Modelo 📊\n",
            "🎯 Exactitud (`Accuracy`): 0.9948\n",
            "📉 Error Cuadrático Medio (`MSE`): 0.0052\n",
            "--------------------------------------------------\n",
            "📢 Gracias por analizar el modelo. Ajusta los parámetros para mejorar los resultados! 🚀\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n📊 🔮 **Análisis del Modelo de Predicción de Préstamos** 🔮 📊\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Mostrar valores únicos de `y_train`\n",
        "print(\"🔍 Exploración de los datos:\")\n",
        "print(f\"✅ Valores únicos en el conjunto de entrenamiento (Y): {np.unique(y_train)}\\n\")\n",
        "\n",
        "# Obtener predicciones\n",
        "y_pred = np.argmax(best_model.predict(X_test), axis=-1)\n",
        "y_true = np.argmax(y_test, axis=-1)\n",
        "\n",
        "# Mostrar comparación de predicción vs valor real en algunas muestras\n",
        "print(\"📈 Comparación entre Predicción y Realidad del modelo de Aprobacion de prestamos:\")\n",
        "print(\"-\" * 50)\n",
        "N = 10  # Número de muestras a imprimir\n",
        "for i in range(min(N, len(y_test))):\n",
        "  print(f\"🔹 Sample {str(i+1).zfill(2)} : 🏦 Predicted={y_pred[i]} | Real={y_true[i]}\")\n",
        "print(\"\")\n",
        "\n",
        "# Calcular métricas de rendimiento\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"📊  Resumen del Rendimiento del Modelo 📊\")\n",
        "print(f\"🎯 Exactitud (`Accuracy`): {accuracy:.4f}\")\n",
        "print(f\"📉 Error Cuadrático Medio (`MSE`): {mse:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "print(\"📢 Gracias por analizar el modelo. Ajusta los parámetros para mejorar los resultados! 🚀\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H73xQBSzmmp5"
      },
      "source": [
        "## Extra: Guardmanos el mejor modelo generado para uso posterior\n",
        "\n",
        "Esto lo elaboramos con fin de guardar la informacion del modelo mejor entrenado para casos futuros asi como un respaldo y no tenerlo en memoria RAM, sino teniendo un elemento en almacenamiento persistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4PHVjfpmiO_",
        "outputId": "bddb40d5-17cf-48df-b8ef-cab130bc16fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Modelo guardado exitosamente en 'modelo_prestamos.pkl'\n",
            "✅ Scaler guardado exitosamente en 'scaler.pkl'\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(best_model, \"modelo_prestamos.pkl\")\n",
        "\n",
        "# Guardar el Scaler en un archivo para el programa de interfaz humano\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "\n",
        "print(\"✅ Modelo guardado exitosamente en 'modelo_prestamos.pkl'\")\n",
        "print(\"✅ Scaler guardado exitosamente en 'scaler.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTIC4cLrhFCI"
      },
      "source": [
        "## 6. Programa para prueba del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibS1AWSXiRFL",
        "outputId": "dc97e681-ddb5-468c-f4f0-77351f1c7290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔮 Sistema de predicción de aprobación de préstamos\n",
            "\n",
            "\n",
            "📌 Descripción del sistema:\n",
            "🔹 Basado en una red neuronal de nivel básico con una capa profunda.\n",
            "🔹 Usa `RandomSearch` para la optimización de hiperparámetros.\n",
            "🔹 Modelo secuencial con activación `ReLU` y salida `softmax`.\n",
            "🔹 Función de pérdida: `categorical_crossentropy`.\n",
            "🔹 Optimizado con `Adam`, ajustando la tasa de aprendizaje.\n",
            "\n",
            "📊 Hiperparámetros:\n",
            "🔹 Número de unidades en la capa oculta: 32 a 512.\n",
            "🔹 Tasa de aprendizaje: 0.0001 a 0.01.\n",
            "🔹 Evaluación basada en `val_accuracy`.\n",
            "🔹 Máximo de 5 pruebas y 3 ejecuciones por prueba.\n",
            "\n",
            "👥 Autores del sistema:\n",
            "🔹 Daniel Bortot\n",
            "🔹 Hualong Chiang\n",
            "🔹 Alfredo Fung\n",
            "--------------------------------------------------\n",
            "📝 Por favor, ingrese los siguientes datos:\n",
            "⚠️ Error: Estado de empleo inválido. Debe ser 'employed' o 'unemployed'.\n",
            "--------------------------------------------------\n",
            "✅ Ingreso de datos exitosamente\n",
            "\n",
            "📊📊📊📊📊 INICIO DEL PROCESO DE PREDICCIÓN 📊📊📊📊📊\n",
            "--------------------------------------------------\n",
            "🔄 Preparando los datos para la predicción...\n",
            "\n",
            "🔎 Palabras clave más frecuentes: ['start', 'invest']\n",
            "🚀 Columnas originales del scaler: ['Text' 'Income' 'Credit_Score' 'Loan_Amount' 'DTI_Ratio'\n",
            " 'Employment_Status']\n",
            "📥 Datos transformados correctamente.\n",
            "🔍 Ajustando los datos para la predicción...\n",
            "✅ Datos listos para la predicción.\n",
            "\n",
            "🤖 Ejecutando el modelo de predicción... 🕒\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "✅ Predicción completada.\n",
            "\n",
            "--------------------------------------------------\n",
            "📊 Resultados del análisis:\n",
            "❌ Probabilidad de rechazo: 100.00%\n",
            "✅ Probabilidad de aprobación: 0.00%\n",
            "\n",
            "📌 Evaluando la decisión final del sistema...\n",
            "\n",
            "🏦 Decisión del sistema: ❌ Préstamo rechazado\n",
            "\n",
            "--------------------------------------------------\n",
            "📢 Gracias por utilizar nuestro sistema de predicción de préstamos. ¡Esperamos haber sido de ayuda!\n"
          ]
        }
      ],
      "source": [
        "import joblib  # Para cargar el modelo previamente entrenado\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Cargar el modelo de clasificación previamente entrenado\n",
        "modelo = joblib.load(\"modelo_prestamos.pkl\")\n",
        "\n",
        "# Hacemos un ciclo while para validar c/u de los campos de tal forma que se entregen los datos al modelo siendo validos\n",
        "def solicitar_datos():\n",
        "    def obtener_texto():\n",
        "        while True:\n",
        "            texto = input(\"📊 Ingrese el motivo del préstamo que quiere solicitar: \").strip().lower()\n",
        "            if texto:\n",
        "                return texto\n",
        "            print(\"⚠️ Error: El motivo del préstamo no puede estar vacío.\")\n",
        "\n",
        "    def obtener_flotante(mensaje):\n",
        "        while True:\n",
        "            try:\n",
        "                valor = float(input(mensaje))\n",
        "                if valor > 0:  # Verifica que el número sea positivo\n",
        "                    return valor\n",
        "                print(\"⚠️ Error: Ingrese un número positivo.\")\n",
        "            except ValueError:\n",
        "                print(\"⚠️ Error: Ingrese un valor numérico válido.\")\n",
        "\n",
        "    def obtener_puntaje_credito():\n",
        "        while True:\n",
        "            try:\n",
        "                puntaje = int(input(\"📊 Ingrese su puntaje de crédito: \"))\n",
        "                if puntaje > 0:  # Verifica que el número sea positivo\n",
        "                    return puntaje\n",
        "                print(\"⚠️ Error: El puntaje debe ser un número positivo.\")\n",
        "            except ValueError:\n",
        "                print(\"⚠️ Error: Ingrese un número entero válido.\")\n",
        "\n",
        "    def obtener_empleo():\n",
        "        opciones_validas = [\"employed\", \"unemployed\"]\n",
        "        while True:\n",
        "            empleo = input(\"👔 Ingrese su estado de empleo (employed, unemployed): \").strip().lower()\n",
        "            if empleo in opciones_validas:\n",
        "                return empleo\n",
        "            print(\"⚠️ Error: Estado de empleo inválido. Debe ser 'employed' o 'unemployed'.\")\n",
        "\n",
        "    # Solicitar datos con validaciones\n",
        "    texto = obtener_texto()\n",
        "    ingreso = obtener_flotante(\"💰 Ingrese sus ingresos mensuales: \")\n",
        "    puntaje_credito = obtener_puntaje_credito()\n",
        "    monto_prestamo = obtener_flotante(\"🏦 Ingrese el monto del préstamo solicitado: \")\n",
        "    dti_ratio = obtener_flotante(\"🔄 Ingrese su ratio DTI (deuda/ingresos): \")\n",
        "    empleo = obtener_empleo()\n",
        "    print(\"-\" * 50)\n",
        "    print(\"✅ Ingreso de datos exitosamente\")\n",
        "\n",
        "\n",
        "    return [texto, ingreso, puntaje_credito, monto_prestamo, dti_ratio, empleo]\n",
        "\n",
        "def predecir_aprobacion(datos):\n",
        "  if not datos:\n",
        "    print(\"⚠️ No hay datos disponibles. Verifica la entrada.\")\n",
        "\n",
        "  print(\"\\n\" + \"📊\" * 5 + \" INICIO DEL PROCESO DE PREDICCIÓN \" + \"📊\" * 5)\n",
        "  print(\"-\" * 50)\n",
        "  print(\"🔄 Preparando los datos para la predicción...\\n\")\n",
        "\n",
        "  # Transformar los datos de entrada\n",
        "  datos_transformados = transformar_datos(datos)\n",
        "  print(\"📥 Datos transformados correctamente.\")\n",
        "\n",
        "  # Convertir a numpy array asegurando la forma adecuada\n",
        "  print(\"🔍 Ajustando los datos para la predicción...\")\n",
        "  datos_array = np.array(datos_transformados, dtype=np.float32).reshape(1, -1)\n",
        "  print(\"✅ Datos listos para la predicción.\\n\")\n",
        "\n",
        "  # Realizar la predicción\n",
        "  print(\"🤖 Ejecutando el modelo de predicción... 🕒\")\n",
        "  prediccion = modelo.predict(datos_array)\n",
        "  print(\"✅ Predicción completada.\\n\")\n",
        "  print(\"-\" * 50)\n",
        "\n",
        "  # Imprimir resultados con formato más claro\n",
        "  probabilidad_aprobacion = prediccion[0][0] * 100  # Convertir a porcentaje\n",
        "  probabilidad_rechazo = prediccion[0][1] * 100  # Convertir a porcentaje\n",
        "\n",
        "  print(\"📊 Resultados del análisis:\")\n",
        "  print(f\"❌ Probabilidad de rechazo: {probabilidad_rechazo:.2f}%\")\n",
        "  print(f\"✅ Probabilidad de aprobación: {probabilidad_aprobacion:.2f}%\\n\")\n",
        "\n",
        "  # Determinar el resultado final\n",
        "  print(\"📌 Evaluando la decisión final del sistema...\\n\")\n",
        "  resultado = \"❌ Préstamo rechazado\" if np.argmax(prediccion) == 1 else \"✅ Préstamo aprobado\"\n",
        "  print(f\"🏦 Decisión del sistema: {resultado}\\n\")\n",
        "\n",
        "  print(\"-\" * 50)\n",
        "  print(\"📢 Gracias por utilizar nuestro sistema de predicción de préstamos. ¡Esperamos haber sido de ayuda!\")\n",
        "\n",
        "def transformar_datos(datos) -> list:\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    # Crear un DataFrame temporal\n",
        "    df = pd.DataFrame([datos], columns=[\"Text\", \"Income\", \"Credit_Score\", \"Loan_Amount\", \"DTI_Ratio\", \"Employment_Status\"])\n",
        "\n",
        "    keywords=[\"start\", \"invest\"]\n",
        "\n",
        "    print(\"🔎 Palabras clave más frecuentes:\", keywords)\n",
        "\n",
        "    # Función para marcar si un texto contiene palabras clave\n",
        "    def contains_keywords(text, keywords):\n",
        "      text = text.lower()\n",
        "      return 1 if any(word in text for word in keywords) else 0\n",
        "\n",
        "    # Funcion para clasificar a las personas de empleados y no empleados\n",
        "    def is_employed(person):\n",
        "      if(person==\"employed\"):\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "    df = df.copy() # Se coloca la copia para evitar que se lance el error al filtrar los datos\n",
        "\n",
        "    # Se filtran los datos del texto por palabras claves\n",
        "    df[\"Text\"] = df[\"Text\"].apply(lambda x: contains_keywords(x, keywords))\n",
        "\n",
        "    # Se filtran los datos del estatus de empleo por palabras claves\n",
        "    df[\"Employment_Status\"] = df[\"Employment_Status\"].apply(lambda x: is_employed(x))\n",
        "\n",
        "    # Normalizar los datos numéricos\n",
        "    scaler = joblib.load(\"scaler.pkl\")  # Cargar el escalador previamente entrenado\n",
        "\n",
        "    print(\"🚀 Columnas originales del scaler:\", scaler.feature_names_in_)\n",
        "\n",
        "    # Escalar los valores\n",
        "    df_transformado = scaler.transform(df)\n",
        "\n",
        "    return df_transformado.tolist()\n",
        "\n",
        "\n",
        "# 🚀 Programa de interacción\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"\\n🔮 Sistema de predicción de aprobación de préstamos\")\n",
        "  print(\"\\n\")\n",
        "  print(\"📌 Descripción del sistema:\")\n",
        "  print(\"🔹 Basado en una red neuronal de nivel básico con una capa profunda.\")\n",
        "  print(\"🔹 Usa `RandomSearch` para la optimización de hiperparámetros.\")\n",
        "  print(\"🔹 Modelo secuencial con activación `ReLU` y salida `softmax`.\")\n",
        "  print(\"🔹 Función de pérdida: `categorical_crossentropy`.\")\n",
        "  print(\"🔹 Optimizado con `Adam`, ajustando la tasa de aprendizaje.\")\n",
        "\n",
        "  print(\"\\n📊 Hiperparámetros:\")\n",
        "  print(\"🔹 Número de unidades en la capa oculta: 32 a 512.\")\n",
        "  print(\"🔹 Tasa de aprendizaje: 0.0001 a 0.01.\")\n",
        "  print(\"🔹 Evaluación basada en `val_accuracy`.\")\n",
        "  print(\"🔹 Máximo de 5 pruebas y 3 ejecuciones por prueba.\")\n",
        "\n",
        "  print(\"\\n👥 Autores del sistema:\")\n",
        "  print(\"🔹 Daniel Bortot\")\n",
        "  print(\"🔹 Hualong Chiang\")\n",
        "  print(\"🔹 Alfredo Fung\")\n",
        "  print(50*\"-\")\n",
        "  print(\"📝 Por favor, ingrese los siguientes datos:\")\n",
        "  datos_usuario = solicitar_datos()\n",
        "  predecir_aprobacion(datos_usuario)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvoqAksdh5hP"
      },
      "source": [
        "## 7. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhpN8r77h9Pa"
      },
      "source": [
        "### Resume los principales hallazgos.\n",
        "\n",
        "Consideramos que el modelo es efectivo porque cumple con la propuesta del laboratorio establecido. Ademas de hacer una interfaz humano computador decente para que sea mas amigable para el usuario.\n",
        "\n",
        "Consideramos que una de las mejoras para hacer es el uso de un sistema o\n",
        "graficas que te muestran aproximadamente cuales son los datos que apuebas asi como cuales datos no aprueban con su debida grafica de los indicacores del excel.\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "- Grafica de tasa promedio de RATIO de DTI rechazada\n",
        "\n",
        "- Grafica de tasa de aprobaciones de acuerdo al ingreso\n",
        "\n",
        "- Media de puntaje crediticio aprobado\n",
        "\n",
        "- Tasa de prestamos rechazados por monto solicitado fuera de los parametros\n",
        "\n",
        "\n",
        "Indica si el modelo fue efectivo en la tarea propuesta y qué mejoras se pueden hacer.\n",
        "\n",
        "### Desafios encontrados:\n",
        "Menciona dificultades en el diseño, entrenamiento o filtrado de datos.\n",
        "\n",
        "Filtrado de los datos\n",
        "\n",
        "- Uso de los strings en campos como texto:\n",
        "\n",
        "  Esto en especial en: ***Text,Employment_Status y Approval*** porque estos campos tenian valores que debian ser evaluados pero el modelo de la libreria de **keras**. Haciendo que diera errores. La mejor forma que encontramos para el procesamiento de los datos fue a traves de la siguiente linea de codigo:\n",
        "\n",
        "  (Para mas detalles ver el paso 1)\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "...\n",
        "# De esta forma se hace una asignacion automatica de los valores de string\n",
        "# fuente: la comunidad de stackoverflow\n",
        "keywords=[\"start\", \"invest\"]\n",
        "\n",
        "print(\"🔎 Palabras clave más frecuentes:\", keywords)\n",
        "\n",
        "# Función para marcar si un texto contiene palabras clave\n",
        "def contains_keywords(text, keywords):\n",
        "    text = text.lower()\n",
        "    return 1 if any(word in text for word in keywords) else 0\n",
        "\n",
        "# Funcion para clasificar a las personas de empleados y no empleados\n",
        "def is_employed(person):\n",
        "  if(person==\"employed\"):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# Aplicar la función al DataFrame\n",
        "X = X.copy() # Se coloca la copia para evitar que se lance el error al filtrar los datos\n",
        "X[\"Text\"] = dataframe[\"Text\"].apply(lambda x: contains_keywords(x, keywords))\n",
        "\n",
        "X[\"Employment_Status\"] = dataframe[\"Employment_Status\"].apply(lambda x: is_employed(x))\n",
        "\n",
        "...  \n",
        "```\n",
        "\n",
        "> 📌 **Nota Importante:**  \n",
        "> La librería requiere **valores numéricos obligatoriamente** para funcionar correctamente.  \n",
        "> Sin este manejo, la transformación fallaría y el sistema no aceptaría los datos.\n",
        "> Este fallo tambien nos paso haciendo la interfaz para los usuarios en dichos campos de nuevo ***Text,Employment_Status y Approval***\n",
        "\n",
        "\n",
        "\n",
        "- Normalización de los datos: Debido a que al manejar datos como es el caso de ingresos que pueden pasar de 10.000 a 1.000 o incluso mas como 100.000$ termina siendo montos muy grandes que deben de ser tratados y manejados. La mejor forma que encontramos para el procesamiento de los datos fue a traves de la siguiente linea de codigo:\n",
        "\n",
        "  (Para mas detalles ver el paso 3)\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split #separador de casos para entrenamiento y prueba\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "...\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Se normalizan los datos a entrenar\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "...\n",
        "```\n",
        "\n",
        "> 📌 **Nota Importante:**  \n",
        "> De esa forma nos ahorramos el problema de la gran diferencia entre los datos\n",
        "\n",
        "### Propón mejoras futuras.\n",
        "\n",
        "Sugerir posibles optimizaciones en el modelo y en la implementación del programa.\n",
        "\n",
        "De acuerdo a la IA la clasificacion de la informacion deberia de hacercse usando una IA de analicis de lenguaje, de forma que el genere esas relaciones de las palabras claves.\n",
        "\n",
        "Otra opcion como dejamos en la documentacion en el paso 2\n",
        "\n",
        "```python\n",
        "...\n",
        "# Filtrar solo los textos aprobados\n",
        "approved_texts = \" \".join(dataframe[dataframe[\"Approval\"] == \"Approved\"][\"Text\"]).lower()\n",
        "\n",
        "# Limpiar texto (eliminar signos de puntuación)\n",
        "approved_texts = re.sub(r'[^\\w\\s]', '', approved_texts)\n",
        "\n",
        "# Separar palabras y contar frecuencia\n",
        "word_counts = Counter(approved_texts.split())\n",
        "\n",
        "# Otro criterio\n",
        "# Obtener las 5 palabras más comunes de los textos aprobados En caso que se quiera hacer con ese criterio\n",
        "# Si se quiere hacer para los mas comunes\n",
        "\n",
        "# keywords = [word for word, count in word_counts.most_common(5)]\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "> 💡 **Idea:**  \n",
        "> Hacer que los datos de las palabras mas encontradas en el documento guardarlo en otro csv o en un txt y de ahi tomarlo como los ***keywords*** para poder sacar los datos de forma dinamica y haga el criterio de acuerdo a los datos el propio modelo,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM0eDjsydLV_"
      },
      "source": [
        "# Criterio de Evaluación\n",
        "\n",
        "| 1-4: Insuficiente | 5-8: En Desarrollo | 9-12: Satisfactorio | 13-16: Bueno | 17-20: Sobresaliente |\n",
        "\n",
        "| 1. Diseño y Entrenamiento del Modelo MLP (8 puntos) | El modelo no se implementa o presenta errores significativos. No se evidencia entrenamiento o el rendimiento es muy pobre. (1-2) | El modelo se implementa con errores y el entrenamiento es básico. El rendimiento es bajo y la comprensión de los hiperparámetros es limitada. (3-5) | Se implementa un modelo MLP funcional y se realiza un entrenamiento básico. El rendimiento es aceptable, y se intenta ajustar algunos hiperparámetros. (6-8) | Se diseña e implementa un modelo MLP robusto. Se realiza un entrenamiento adecuado y se exploran varios hiperparámetros para mejorar el rendimiento. (9-12) | Se diseña e implementa un modelo MLP altamente efectivo y optimizado. Se realiza un entrenamiento exhaustivo y se aplica un tuning de hiperparámetros sistemático utilizando Keras Tuner u otras técnicas avanzadas, logrando un rendimiento sobresaliente. (13-16) | Se demuestra un diseño excepcional del modelo MLP, con una justificación clara de la arquitectura y las funciones de activación. El entrenamiento es riguroso y la optimización de hiperparámetros es exhaustiva y bien documentada, resultando en un modelo con un rendimiento superior y generalización robusta. (17-20) |\n",
        "\n",
        "\n",
        "| 2. Filtrado de Datos para Negocios (4 puntos) | No se realiza ningún filtrado de datos o el intento es incorrecto. (1) | Se intenta filtrar los datos, pero el método es ineficiente o incorrecto, incluyendo datos irrelevantes. (2) | Se realiza un filtrado básico de los datos, identificando algunas instancias relevantes para negocios, pero aún quedan datos no deseados. (3) | Se implementa una estrategia efectiva para filtrar los datos, identificando la mayoría de las instancias relevantes para negocios utilizando palabras clave como \"invest\" y \"start\". (4) | Se demuestra una comprensión profunda de la tarea de filtrado, implementando una estrategia sofisticada y precisa que aísla eficazmente los datos relevantes para préstamos de negocios, justificando las palabras clave y la lógica utilizada. (4) |\n",
        "\n",
        "\n",
        "| 3. Programa de Interacción con el Usuario (4 puntos) | El programa no se implementa o falla al recibir entradas. No se manejan errores de entrada. (1) | Se implementa un programa básico que solicita entradas, pero no maneja entradas inválidas y puede arrojar errores. La recomendación es simple o inexistente. (2) | El programa solicita entradas clave y proporciona una recomendación básica. Se intenta manejar algunas entradas inválidas, pero no de forma exhaustiva. (3) | El programa guía al usuario para ingresar los datos clave de manera clara y amigable. Implementa una validación robusta para corregir entradas inválidas sin errores. La recomendación es clara y se basa en la salida del modelo. (4) | El programa ofrece una experiencia de usuario intuitiva y robusta. La validación de entradas es exhaustiva y proporciona retroalimentación útil al usuario. La recomendación es detallada y contextualizada, demostrando una integración perfecta con el modelo predictivo. (4) |\n",
        "\n",
        "\n",
        "| 4. Calidad y Claridad del Código (4 puntos) | El código es ilegible, sin comentarios o con comentarios incorrectos. No se identifica el código generado por IA. (1) | El código es difícil de seguir, con pocos comentarios o comentarios vagos. La identificación del código generado por IA es inconsistente o ausente. (2) | El código es generalmente legible y contiene algunos comentarios útiles. Se intenta identificar el código generado por IA, pero la explicación puede ser superficial. (3) | Todo el código está debidamente comentado, explicando la lógica de cada sección y variable. Se identifica claramente el código generado por IA y se explica su funcionamiento línea por línea. (4) | El código demuestra una organización y claridad excepcionales. Los comentarios son detallados y perspicaces, facilitando la comprensión del flujo y la lógica del programa. La identificación y explicación del código generado por IA son exhaustivas y demuestran un entendimiento profundo del código. (4) |\n",
        "\n",
        "Puntuación Total Posible: 20 puntos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO7EZP5QBCsT"
      },
      "source": [
        "##Tips del Profesor\n",
        "\n",
        "NT: Hay posiblidad que el modelo quede muy pequeño y hay que determinar que hacer en caso que haya underfetting\n",
        "\n",
        "Es un modelo de clasificacion\n",
        "\n",
        "1. ¿Cuantas personas quieren invertir en el negocio? 500 es buen camino\n",
        "2. ¿ Hay outlayers en las variables? ¿ En cuales variables? ¿Como las voy a calcular?\n",
        "(la libreria del profesor tiene una funcion para saber cuales son los outlayers)\n",
        "\n",
        "3. ¿Que hago con las variables cualitativas? (categorias) Ej creo hacer que categoria 1 = 1 y asi\n",
        "\n",
        "**OJO: No puede haber un desbalanceo de los datos osea 2 datos 50/50 3 datos 33/33/33**\n",
        "\n",
        "Librerias recomendadas: Keras, sidekid, kerastunner\n",
        "\n",
        "**PISTA: Lo que hable de expandir, mejorar, etc son elmentos que probablemnte si se tengas que prestar**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
